{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'app', 'belly', 'best', 'came', 'cat', 'chrome', 'climbing', 'eating', 'extension', 'face', 'feedback', 'google', 'impressed', 'incredible', 'key', 'kitten', 'kitty', 'little', 'map', 'merley', 'ninja', 'open', 'photo', 'play', 'promoter', 'restaurant', 'smiley', 'squooshy', 'tab', 'taken', 'translate', 've']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 2\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.45217213],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.49414046, 0.        ],\n",
       "       [0.        , 0.74849032],\n",
       "       [0.        , 0.5964714 ],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.52368298, 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nmf.fit_transform(tfidf)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = nmf.components_\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tfidf - np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic #1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Modelling with NMF:\n",
      "Topic 0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic 1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n",
      " ---- \n",
      "Topic Modelling with LDA\n",
      "Topic 0:\n",
      "google smiley translate restaurant tab promoter eating face feedback kitty\n",
      "Topic 1:\n",
      "cat best taken merley belly kitten squooshy ve ninja climbing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# no_features = 100\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1).fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "print('Topic Modelling with NMF:')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print(' ---- ')\n",
    "print('Topic Modelling with LDA')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 1.17360019 0.\n",
      "  1.09861229 0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.         0.\n",
      "  0.         1.09861229 1.09861229 1.09861229]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]\n",
      "Word Index:\n",
      "{'this': 1, 'is': 2, 'the': 3, 'document': 4, 'first': 5, 'second': 6, 'and': 7, 'third': 8, 'one': 9}\n",
      "Word Counts:\n",
      "OrderedDict([('this', 4), ('is', 4), ('the', 4), ('first', 2), ('document', 4), ('second', 1), ('and', 1), ('third', 1), ('one', 1)])\n",
      "Document Count:\n",
      "4\n",
      "Words in Doc:\n",
      "defaultdict(<class 'int'>, {'is': 4, 'first': 2, 'this': 4, 'document': 3, 'the': 4, 'second': 1, 'one': 1, 'and': 1, 'third': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "documents = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(documents)\n",
    "mat_texts = tok.texts_to_matrix(documents, mode='tfidf')\n",
    "print(mat_texts)\n",
    "X = tok.texts_to_sequences(documents)\n",
    "print(X)\n",
    "print('Word Index:')\n",
    "print(tok.word_index)\n",
    "print('Word Counts:')\n",
    "print(tok.word_counts)\n",
    "print('Document Count:')\n",
    "print(tok.document_count)\n",
    "print('Words in Doc:')\n",
    "print(tok.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification + Keras:\n",
    "\n",
    "https://www.kaggle.com/psyhoo/spam-sms-neural-networks-in-keras\n",
    "\n",
    "num_max: The entire vocabulary we have in corpus -> vocab_size\n",
    "\n",
    "max_len: The maximum number of words per row (how many words a ham or spam email has) in corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_to_category_GloVe(keyword_list):\n",
    "    dic = {}\n",
    "    with codecs.open('glove.840B.300d.txt', 'r') as f:\n",
    "    # with codecs.open('glove.6B.300d.txt', 'r', 'utf-8') as f:\n",
    "        for c, r in enumerate(f):\n",
    "            sr = r.split()\n",
    "            # if sr[0] in keyword_list + category_list:\n",
    "            if sr[0] in [i.encode() for i in keyword_list]:\n",
    "                # print(sr[0])\n",
    "                dic[sr[0]] = [float(i) for i in sr[1:]]\n",
    "                # print(c)\n",
    "                if len(dic) == len(keyword_list):\n",
    "                    break\n",
    "    category_list = pickle.load(open(\"category2.p\", \"rb\"))\n",
    "    category = {}\n",
    "    for i in keyword_list:\n",
    "        distance = []\n",
    "        for j in category_list:\n",
    "            distance.append([j, np.linalg.norm(np.array(dic[i])-np.array(category_list[j]))])\n",
    "        di = [s[0] for s in distance]\n",
    "        mi = [s[1] for s in distance]\n",
    "        idx = mi.index(min(mi))\n",
    "        category[i] = di[idx]\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keyword_to_category_GloVe([u'runner', u'pizza', u'physics', u'adidas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: Which sentences is what document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04624001 0.95375999]\n",
      " [0.06901926 0.93098074]\n",
      " [0.91056403 0.08943597]\n",
      " [0.0516827  0.9483173 ]\n",
      " [0.08450956 0.91549044]\n",
      " [0.85683624 0.14316376]\n",
      " [0.91108699 0.08891301]\n",
      " [0.92322557 0.07677443]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "\n",
    "# the dataset to predict on (first two samples were also in the training set so one can compare)\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "# Vectorize the training set using the model features as vocabulary\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=2).fit(tf)\n",
    "\n",
    "# transform method returns a matrix with one line per document, columns being topics weight\n",
    "predict = lda.transform(tf)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62680148, 0.37319852]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf_vectorizer.transform(['cat is kitty'])\n",
    "lda.transform(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
